{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "#from keras.applications.vgg16 import VGG16\n",
    "\n",
    "#from keras.applications.xception import Xception\n",
    "from keras.utils import Sequence\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.preprocessing import image\n",
    "#from keras.applications.vgg16 import preprocess_input\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "import numpy as np\n",
    "from keras.models import Model,Sequential\n",
    "from keras.layers import Dense,Activation,Dropout,Flatten,AveragePooling2D\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from keras.models import model_from_json\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_csv(\"../../datasets/Diabatic_Retinopathy/trainLabels.csv\")\n",
    "#print(a)\n",
    "size1=300\n",
    "size2=200\n",
    "channels=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['image']=='10207_left']['level']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "basemodel=ResNet50(weights='imagenet',include_top=False,input_shape=(size2,size1,channels))\n",
    "basemodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "model=Sequential()\n",
    "model.add(basemodel)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1000,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adadelta',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in model.layers:\n",
    "    print(i.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbt=[]\n",
    "train=list(os.walk('../../datasets/Diabatic_Retinopathy/train'))[0][2]\n",
    "train2=list(map(lambda x:x.split('.')[0],train))\n",
    "print(len(train2))\n",
    "for j in train2:\n",
    "    ind=data[data['image']==j][\"level\"]\n",
    "            \n",
    "    if(list(ind)[0]>0):\n",
    "        lbt.append(1)\n",
    "    else:\n",
    "        lbt.append(0)\n",
    "print(len(lbt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "images=[]\n",
    "label=[]\n",
    "train=[]\n",
    "mn1=5000\n",
    "mn2=5000\n",
    "count=0\n",
    "\n",
    "n_classes=2\n",
    "a = np.arange(n_classes)\n",
    "#print(a)\n",
    "\n",
    "b = np.zeros((n_classes, n_classes))\n",
    "b[np.arange(n_classes), a] = 1\n",
    "print(b.shape)\n",
    "\n",
    "#data=pd.read_csv(\"trainLabels.csv\")\n",
    "batch_size=1\n",
    "count=0\n",
    "k=False\n",
    "t=0\n",
    "flag=False\n",
    "y=[]\n",
    "def generator(batch_size):\n",
    "        while(True):\n",
    "            train=list(os.walk('../../datasets/Diabatic_Retinopathy/train'))[0][2]\n",
    "            print(len(train))\n",
    "        \n",
    "            train2=list(map(lambda x:x.split('.')[0],train))\n",
    "            train.extend(train2)\n",
    "       \n",
    "            for j in train2:\n",
    "                try:\n",
    "                \n",
    "                    img=Image.open('../../datasets/Diabatic_Retinopathy/train/'+j+'.jpeg')\n",
    "                    x=np.array(img.resize((size1,size2)).convert('RGB')).reshape(1,size2,size1,3)\n",
    "                \n",
    "                    ind=data[data['image']==j][\"level\"]\n",
    "            \n",
    "                    if(list(ind)[0]>0):\n",
    "                        ind=1\n",
    "                    else:\n",
    "                        ind=0\n",
    "                    global flag,count,t\n",
    "                    if flag:\n",
    "                        images=np.concatenate((images,x),axis=0)\n",
    "                        label=np.concatenate((label,[b[ind]]),axis=0)\n",
    "                    else:\n",
    "                        flag=True\n",
    "                        images=x\n",
    "                    \n",
    "                        label=np.array([b[ind]])\n",
    "                    \n",
    "                    count+=1\n",
    "                    t+=1  \n",
    "                    #print(\"count=\",count,\" t=\",t)\n",
    "                    if(count==batch_size):\n",
    "                        count=0\n",
    "                        print(t)\n",
    "                        flag=False\n",
    "                        \n",
    "                        yield images,label\n",
    "                    \n",
    "                     \n",
    "                    \n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    continue\n",
    "            \n",
    "\n",
    "ep=(int)(35125/5)+1\n",
    "#print(ep)\n",
    "\n",
    "model.fit_generator(generator(5),epochs=5,steps_per_epoch=ep)\n",
    "\n",
    "model.save_weights(\"../../datasets/Diabatic_Retinopathy/resnet.h5\")\n",
    "#.extend(model.predict_generator(generator(5),steps=ep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"../../datasets/Diabatic_Retinopathy/resnet.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(\"../../datasets/Diabatic_Retinopathy/resnet.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "# load json and create model\n",
    "#json_file = open('../../datasets/Diabatic_Retinopathy/resnet.json', 'r')\n",
    "#model_json = json_file.read()\n",
    "#json_file.close()\n",
    "#model = model_from_json(model_json)\n",
    "# load weights into new model\n",
    "model.load_weights(\"../../datasets/Diabatic_Retinopathy/resnet.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"../../datasets/Diabatic_Retinopathy/resnet.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im1=np.load(\"../../datasets/Diabatic_Retinopathy/resnet.npy\")\n",
    "im2=np.load(\"../../datasets/Diabatic_Retinopathy/resnet1.npy\")\n",
    "lb1=np.load(\"../../datasets/Diabatic_Retinopathy/label.npy\")\n",
    "lb2=np.load(\"../../datasets/Diabatic_Retinopathy/label1.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(im.shape,lb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"../../datasets/Diabatic_Retinopathy/trainLabels.csv\")\n",
    "keys, values = zip(*images.items())\n",
    "df=data[data['image'].isin(keys)]\n",
    "dic={'image':keys,'values':values}\n",
    "df2=pd.DataFrame(dic)\n",
    "dt=df.set_index('image').join(df2.set_index('image'))\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "vec=OneHotEncoder()\n",
    "br=np.array(list(map(lambda x:[x],list(dt[\"level\"]))))\n",
    "\n",
    "labels=vec.fit_transform(br).toarray()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=np.array(list(dt['values']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.shape,labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys, values = zip(*images.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic={'image':keys,'values':values}\n",
    "df2=pd.DataFrame(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt=df.set_index('image').join(df2.set_index('image'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "vec=OneHotEncoder()\n",
    "br=np.array(list(map(lambda x:[x],list(dt[\"level\"]))))\n",
    "\n",
    "labels=vec.fit_transform(br).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features=np.array(list(dt['values']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(dt.index)[87]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images['194_right']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt=dt[dt.index !='194_right' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl='RSS50'\n",
    "features=np.load('../../datasets/Diabatic_Retinopathy/'+mdl+'features.npy')\n",
    "labels=np.load('../../datasets/Diabatic_Retinopathy/label.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.shape,labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(activations)\n",
    "model=Sequential()\n",
    "\n",
    "model.add(Dense(1000,input_dim=features.shape[1],name='fc',activation='sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(labels.shape[1],activation='softmax',name='prediction'))\n",
    "model.compile(optimizer='adadelta',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model.fit(x=features,y=labels,batch_size=100,epochs=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(base_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(x.shape)\n",
    "for layer in base_model.layers:\n",
    "    name=layer.name\n",
    "    print(name)\n",
    "    model1 = Model(inputs=base_model.input, outputs=base_model.get_layer(layer.name).output)\n",
    "    try:\n",
    "        print(base_model.get_layer(layer.name))\n",
    "        try:\n",
    "            print(base_model.get_layer(layer.name).weights[0].shape,base_model.get_layer(layer.name).weights[1].shape)\n",
    "        except:\n",
    "            print(\"1\")\n",
    "        print(\"strides=\",base_model.get_layer(layer.name).strides)\n",
    "        print(\"window=\",base_model.get_layer(layer.name).pool_size)\n",
    "    except:\n",
    "        print(\"2\")\n",
    "    print(\"3\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net:\n",
    "    def __init__(self):\n",
    "       \n",
    "        self.blobs={}\n",
    "        self.params={}\n",
    "    def param(self,data):\n",
    "        if(data==[]):\n",
    "            return data\n",
    "        if data.ndim == 4:\n",
    "            data = np.transpose(data, (3, 2, 0, 1))\n",
    "        elif data.ndim == 2:\n",
    "            data = np.transpose(data)\n",
    "        else:\n",
    "            raise(ValueError, 'Well this is unexpected...')\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "    def blob(self,data):\n",
    "        if data.ndim == 4:\n",
    "            data = np.transpose(data, (0, 3, 1, 2))\n",
    "        elif data.ndim == 2:\n",
    "            data = np.transpose(data)\n",
    "        else:\n",
    "            raise (ValueError, 'Well this is unexpected...')\n",
    "\n",
    "        return data\n",
    "   \n",
    "    def update(self,name,blob,param):\n",
    "       \n",
    "        self.blobs[name]=Data(self.blob(blob))\n",
    "        if param!=[]:\n",
    "            self.params[name]=[Data(self.param(param))]\n",
    "        else:\n",
    "            self.params[name]=[]\n",
    "\n",
    "class Data:\n",
    "\n",
    "    def __init__(self, d):\n",
    "        self.data = d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VGG16(points):\n",
    "    #print('predictions=',points.shape)\n",
    "    points = layers.fc(points=points, layer='prediction', prevLayer='fc')\n",
    "    \n",
    "    points = layers.fc(points=points, layer='fc', prevLayer='block5_pool')\n",
    "    #print('block5_conv3=',points.shape)\n",
    "    points = layers.pool(points=points, prevLayer='block5_conv3', K=2, S=2)\n",
    "    #print('block5_conv3pool=',points.shape)\n",
    "    points = layers.conv(points=points,  layer='block5_conv3', prevLayer='block5_conv2', K=3, S=1, P=2)\n",
    "    #print('block5_conv1=',points.shape)\n",
    "    points = layers.conv(points=points,  layer='block5_conv2', prevLayer='block5_conv1', K=3, S=1, P=2)\n",
    "    #print('block5_conv3=',points.shape)\n",
    "    points = layers.conv(points=points,  layer='block5_conv1', prevLayer='block4_pool', K=3, S=1, P=2)\n",
    "    points = layers.pool(points=points, prevLayer='block4_conv3', K=2, S=2)\n",
    "    points = layers.conv(points=points,  layer='block4_conv3', prevLayer='block4_conv2', K=3, S=1, P=2)\n",
    "    points = layers.conv(points=points,  layer='block4_conv2', prevLayer='block4_conv1', K=3, S=1, P=2)\n",
    "    points = layers.conv(points=points,  layer='block4_conv1', prevLayer='block3_pool', K=3, S=1, P=2)\n",
    "    points = layers.pool(points=points, prevLayer='block3_conv3', K=2, S=2)\n",
    "    points = layers.conv(points=points,  layer='block3_conv3', prevLayer='block3_conv2', K=3, S=1, P=2)\n",
    "    points = layers.conv(points=points,  layer='block3_conv2', prevLayer='block3_conv1', K=3, S=1, P=2)\n",
    "    points = layers.conv(points=points,  layer='block3_conv1', prevLayer='block2_pool', K=3, S=1, P=2)\n",
    "    points = layers.pool(points=points, prevLayer='block2_conv2', K=2, S=2)\n",
    "    points = layers.conv(points=points,  layer='block2_conv2', prevLayer='block2_conv1', K=3, S=1, P=2)\n",
    "    points = layers.conv(points=points,  layer='block2_conv1', prevLayer='block1_pool', K=3, S=1, P=2)  \n",
    "    points = layers.pool(points=points, prevLayer='block1_conv2', K=2, S=2)\n",
    "    points = layers.conv(points=points,  layer='block1_conv2', prevLayer='block1_conv1', K=3, S=1, P=2)\n",
    "    print(base_model.layers[0].name)\n",
    "    points = layers.conv(points=points,  layer='block1_conv1', prevLayer=base_model.layers[0].name, K=3, S=1, P=2)\n",
    "    points = layers.data(points=points)\n",
    "    #points = layers.data(points=points, inc=inc, resFac=resFac)\n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Fixations:\n",
    "    def __init__(self, net):\n",
    "        self.net = net\n",
    "\n",
    "    def fc(self, points, layer, prevLayer):\n",
    "        #print(\"sadfkm\")\n",
    "        for cr in range(1):\n",
    "            #print(\"points[cr]=\",points[cr])\n",
    "            if (points[cr] != 0):\n",
    "                layer_out = []\n",
    "                # Blob values for prevous layer\n",
    "                #print(\"Sdf\",self.net.blobs[prevLayer].data)\n",
    "                \n",
    "                #print(\"data=\",data)\n",
    "                # Weights for the current layer\n",
    "                param = self.net.params[layer][0].data\n",
    "                print(self.net.blobs[prevLayer].data.ndim )\n",
    "                # If previous layer is not fully connected\n",
    "                if (self.net.blobs[prevLayer].data.ndim == 4):\n",
    "                    print(\"if\")\n",
    "                    data =np.squeeze(self.net.blobs[prevLayer].data[cr,:])\n",
    "                    shape = data.shape\n",
    "                    print(shape,param.shape[1])\n",
    "                    data = np.reshape(data, [param.shape[1], ])\n",
    "                    conv = data*param\n",
    "                    for i in points[cr]:\n",
    "                        # Getting Top positions for each input\n",
    "                        position = np.argmax(conv[i, :])\n",
    "                        layer_out.append(np.unravel_index(position, shape))\n",
    "                else:\n",
    "                    print(\"else\")\n",
    "                    data =np.squeeze(self.net.blobs[prevLayer].data[:,cr])\n",
    "                    conv = data*param\n",
    "                    #print(\"conv=\",conv)\n",
    "                    for i in points[cr]:\n",
    "                        # Getting Top-num activations for each input\n",
    "                        num = np.sum(conv[i, :] > 0)\n",
    "                        #print(\"num=\",num)\n",
    "                        layer_out.extend(np.argsort(conv[i, :])[::-1][:num])\n",
    "                        #print(\"layerout=\",layer_out)\n",
    "                points[cr] = list(set(layer_out))\n",
    "                #print(\"point[cr]1=\",points[cr])\n",
    "        return points\n",
    "\n",
    "    def pool(self, points, prevLayer, K, S):\n",
    "        for cr in range(1):\n",
    "            if (points[cr] != 0):\n",
    "                layer_out = []\n",
    "                for i in points[cr]:\n",
    "                    if not isinstance(i, tuple):\n",
    "                        i = [i] + [0, 0]\n",
    "                    \n",
    "                    if(prevLayer=='res5c_branch2c'):\n",
    "                        i=[i[0],0,0]\n",
    "                    #print(i[1],i[2])\n",
    "                    # Getting the receptive region the pool operates on\n",
    "                    x = (S*i[1], S*i[1] + K)\n",
    "                    y = (S*i[2], S*i[2] + K)\n",
    "                    # Getting most contributing x and y relative to the\n",
    "                    # region being operated on\n",
    "                   \n",
    "                    blob = self.net.blobs[prevLayer].data[cr, i[0], x[0]:x[1], y[0]:y[1]]\n",
    "                    x1, y1 = np.unravel_index(np.argmax(blob), blob.shape)\n",
    "                    layer_out.append((i[0], x[0]+x1, y[0]+y1))\n",
    "                points[cr] = list(set(layer_out))\n",
    "        return points\n",
    "    \n",
    "    def conv(self, points, layer, prevLayer, K, S, P, group=False):\n",
    "        # Only supports group = 2\n",
    "        for cr in range(1):\n",
    "            if (points[cr] != 0):\n",
    "                layer_out = []\n",
    "                for i in points[cr]:\n",
    "                    x = (S*i[1], S*i[1]+K)\n",
    "                    y = (S*i[2], S*i[2]+K)\n",
    "                    flag = False  # To caluculate feature for grouping\n",
    "                    if group:\n",
    "                        p_s = self.net.params[layer][0].data.shape[1]\n",
    "                        b_s = self.net.blobs[prevLayer].data.shape[1]\n",
    "                        if (i[0] >= p_s/2):  # Choosing the group\n",
    "                            flag = True\n",
    "                            if (P != 0):\n",
    "                                data = np.lib.pad(self.net.blobs[prevLayer].data[\n",
    "                                    cr, b_s/2:b_s, :, :], P, 'constant',\n",
    "                                    constant_values=0)[P:-P, x[0]:x[1], y[0]:y[1]]\n",
    "                            else:\n",
    "                                data = self.net.blobs[prevLayer].data[cr, b_s/2:b_s, x[0]:x[1], y[0]:y[1]]\n",
    "                        else:\n",
    "                            if (P != 0):\n",
    "                                data = np.lib.pad(self.net.blobs[prevLayer].data[\n",
    "                                    cr, :b_s/2, :, :], P, 'constant',\n",
    "                                    constant_values=0)[P:-P, x[0]:x[1], y[0]:y[1]]\n",
    "                            else:\n",
    "                                data = self.net.blobs[prevLayer].data[cr, :b_s/2, x[0]:x[1], y[0]:y[1]]\n",
    "                    else:\n",
    "                        if (P != 0):\n",
    "                            data = np.lib.pad(self.net.blobs[prevLayer].data[\n",
    "                                    cr, :, :, :], P, 'constant',\n",
    "                                    constant_values=0)[P:-P, x[0]:x[1], y[0]:y[1]]\n",
    "                        else:\n",
    "                                data = self.net.blobs[prevLayer].data[cr, :, x[0]:x[1], y[0]:y[1]]\n",
    "                    param = self.net.params[layer][0].data[i[0], :, :, :]\n",
    "                    conv = data*param\n",
    "                    # Getting most contributing position\n",
    "                    feature = np.argmax(np.sum(np.sum(conv, axis=2), axis=1))\n",
    "                    if (flag):\n",
    "                        feature += b_s/2\n",
    "                    layer_out.append((feature, x[0], y[0]))\n",
    "                points[cr] = list(set(layer_out))\n",
    "        return points\n",
    "\n",
    "    def data(self, points):\n",
    "        output = []\n",
    "        for cr in range(1):\n",
    "            if (points[cr] != 0):\n",
    "                layer_out = []\n",
    "                # Bringing points back to image size\n",
    "                for i in points[cr]:\n",
    "                    layer_out.append((int(i[1]),\n",
    "                                      int(i[2])))\n",
    "                output.extend(layer_out)\n",
    "        return output\n",
    "    \n",
    "    # Inception layer for GoogLeNet\n",
    "    def inception(self, points, layer, prevLayer, pool=False, out=False, prevLayer2=None):\n",
    "        for cr in range(1):\n",
    "            if (points[cr] != 0):\n",
    "                layer_out = []\n",
    "                for i in points[cr]:\n",
    "                    # Just to handle naming\n",
    "                    if pool:\n",
    "                        add = ''\n",
    "                    else:\n",
    "                        add = '/output'\n",
    "                    # Getting the ranges of each branch in the inception layer\n",
    "                    num_f1 = self.net.blobs[layer+'/1x1'].data.shape[1]\n",
    "                    num_f2 = self.net.blobs[layer+'/3x3'].data.shape[1]\n",
    "                    num_f3 = self.net.blobs[layer+'/5x5'].data.shape[1]\n",
    "                    # Checking if we need to take the 1x1 conv path\n",
    "                    if i[0] < num_f1:\n",
    "                        param = self.net.params[layer+'/1x1'][0].data[i[0], :, 0, 0]\n",
    "                        data = self.net.blobs[prevLayer+add].data[cr,:, i[1], i[2]]\n",
    "                        conv = np.argmax(data*param)\n",
    "                        values = (conv, i[1], i[2])\n",
    "                    # Checking if we need to take the 3x3 conv path\n",
    "                    elif i[0] < num_f1 + num_f2:\n",
    "                        param = self.net.params[layer+'/3x3'][0].data[i[0]-num_f1,:,:,:]\n",
    "                        data = np.lib.pad(self.net.blobs[layer+'/3x3_reduce'].data[cr,:,:,:], 1, 'constant'\n",
    "                                          ,constant_values=0)[1:-1, i[1]:i[1]+3, i[2]:i[2]+3]\n",
    "                        feature = np.argmax(np.sum(np.sum(data*param, axis=2), axis=1))\n",
    "                        param = self.net.params[layer+'/3x3_reduce'][0].data[feature, :, 0, 0]\n",
    "                        data = self.net.blobs[prevLayer+add].data[cr, :, i[1], i[2]]\n",
    "                        conv = np.argmax(data*param)\n",
    "                        values = (conv,i[1],i[2])\n",
    "                    # Checking if we need to take the 5x5 conv path\n",
    "                    elif i[0] < num_f1 + num_f2 + num_f3:\n",
    "                        param = self.net.params[layer+'/5x5'][0].data[i[0]-num_f1-num_f2, :, :, :]\n",
    "                        data =  np.lib.pad(self.net.blobs[layer+'/5x5_reduce'].data[cr,:,:,:], 2, 'constant',\n",
    "                                           constant_values=0)[2:-2,i[1]:i[1]+5,i[2]:i[2]+5]\n",
    "                        feature = np.argmax(np.sum(np.sum(data*param,axis=2),axis=1))\n",
    "                        param = self.net.params[layer+'/5x5_reduce'][0].data[feature,:,0,0]\n",
    "                        data = self.net.blobs[prevLayer+add].data[cr, :, i[1], i[2]]\n",
    "                        conv = np.argmax(data*param)\n",
    "                        values = (conv, i[1], i[2])\n",
    "                    # Otherwise taking the pool path\n",
    "                    else:\n",
    "                        param = self.net.params[layer+'/pool_proj'][0].data[i[0]-num_f1-num_f2-num_f3, :, 0, 0]\n",
    "                        data = self.net.blobs[layer+'/pool'].data[cr, :, i[1], i[2]]\n",
    "                        feature = np.argmax(data*param)\n",
    "                        data = np.lib.pad(self.net.blobs[prevLayer+add].data[cr, :, :, :],1,'constant',\n",
    "                                          constant_values=0)[feature+1, i[1]:i[1]+3, i[2]:i[2]+3]\n",
    "                        x1, y1 = np.unravel_index(np.argmax(data), data.shape)\n",
    "                        values = (feature, i[1]+x1-1, i[2]+y1-1)\n",
    "                    layer_out.append(values)\n",
    "                points[cr] = list(set(layer_out))\n",
    "        return points\n",
    "    \n",
    "    # Residual block for ResNet-101\n",
    "    def res(self, points, layer, prevLayer):\n",
    "        print(prevLayer,\" \",layer)\n",
    "        for cr in range(1):\n",
    "            if (points[cr] != 0):\n",
    "                layer_out = []\n",
    "            flag = False #flag to check if previous block downsampled the inputs\n",
    "            if 'a' in layer:\n",
    "                    flag = True\n",
    "                    # Getting activations from previous residual block\n",
    "                    branch_skip = self.net.blobs['res'+layer+'_branch1'].data[cr]\n",
    "            else:   \n",
    "                   \n",
    "                    branch_skip = self.net.blobs['add_'+prevLayer].data[cr]\n",
    "            # Getting activations from all convolution layers inside the residual block\n",
    "            branch_res_blobs = [self.net.blobs['res'+layer+'_branch2a'].data[cr], self.net.blobs['res'+layer+'_branch2b'].data[cr], self.net.blobs['res'+layer+'_branch2c'].data[cr]]\n",
    "        # Pad input to 3x3 block so the size remains same after conv\n",
    "            branch_res_blobs[0] = np.lib.pad(branch_res_blobs[0], 1 ,'constant',constant_values=0)[1:-1,:,:]\n",
    "            for i in points[cr]:\n",
    "                # To check if the higher activation at a point came from skip or delta branch\n",
    "                if branch_skip[i[0], i[1], i[2]] >= branch_res_blobs[2][i[0], i[1], i[2]]:\n",
    "                        if flag:\n",
    "                                param = self.net.params['res'+layer+'_branch1'][0].data[i[0], :, 0, 0]\n",
    "                                if prevLayer == 'max_pooling2d_2':\n",
    "                                        data = self.net.blobs[prevLayer].data[cr, :, i[1], i[2]]\n",
    "                                        feature = np.argmax(data*param)\n",
    "                                        layer_out.append((feature, i[1], i[2]))\n",
    "                                else:\n",
    "                                        data = self.net.blobs['add_'+prevLayer].data[cr, :, i[1]*2, i[2]*2]\n",
    "                                        feature = np.argmax(data*param)\n",
    "                                        layer_out.append((feature, i[1]*2, i[2]*2))\n",
    "                        else:\n",
    "                                layer_out.append(i)\n",
    "                else:\n",
    "                        param = self.net.params['res'+layer+'_branch2c'][0].data[i[0], :, 0, 0]\n",
    "                        feature = np.argmax(branch_res_blobs[1][:, i[1], i[2]]*param)\n",
    "                        param = self.net.params['res'+layer+'_branch2b'][0].data[feature, :, :, :]\n",
    "                        data = branch_res_blobs[0][:, i[1]:i[1]+3, i[2]:i[2]+3]\n",
    "                        feature = np.argmax(np.sum(np.sum(data*param, axis=2),axis=1))\n",
    "                        x, y = np.unravel_index(np.argmax(data[feature,:,:]),data[feature,:,:].shape)\n",
    "                        if flag:\n",
    "                                param = self.net.params['res'+layer+'_branch2a'][0].data[feature, :, 0, 0]\n",
    "                                if prevLayer =='max_pooling2d_2':\n",
    "                                        feature = np.argmax(self.net.blobs[prevLayer].data[cr, :, i[1]+x-1, i[2]+y-1]*param)\n",
    "                                        layer_out.append((feature, i[1]+x-1, i[2]+y-1))\n",
    "                                else:\n",
    "                                        feature = np.argmax(self.net.blobs['add_'+prevLayer].data[cr, :, (i[1]+x-1)*2, (i[2]+y-1)*2]*param)\n",
    "                                        layer_out.append((feature, (i[1]+x-1)*2, (i[2]+y-1)*2))\n",
    "                        else:\n",
    "                                param = self.net.params['res'+layer+'_branch2a'][0].data[feature, :, 0, 0]\n",
    "                                feature = np.argmax(branch_skip[:,i[1]+x-1,i[2]+y-1]*param)\n",
    "                                layer_out.append((feature, i[1]+x-1, i[2]+y-1))\n",
    "            points[cr] = list(set(layer_out))                        \n",
    "        return points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(net,model,x):\n",
    "    print('update')\n",
    "    \n",
    "    for layer in model.layers[0].layers:\n",
    "        name=layer.name\n",
    "        if \"bn\" in name or \"act\" in name:\n",
    "            continue\n",
    "        print(name)\n",
    "        model1 = Model(inputs=model.layers[0].input, outputs=model.layers[0].get_layer(layer.name).output)\n",
    "        \n",
    "        if len(layer.get_weights())>0:\n",
    "            weights=layer.get_weights()[0]\n",
    "        else:\n",
    "            weights=[]\n",
    "        \n",
    "        tmp=model1.predict(x)\n",
    "        print(tmp.shape)\n",
    "        net.update(name,tmp,weights)\n",
    "    for layer in model.layers:\n",
    "        if(layer.name=='resnet50'):\n",
    "            continue\n",
    "        name=layer.name\n",
    "        print(name)\n",
    "        model1 = Model(inputs=model.input, outputs=model.get_layer(layer.name).output)\n",
    "        if len(layer.get_weights())>0:\n",
    "            weights=layer.get_weights()[0]\n",
    "        else:\n",
    "            weights=[]\n",
    "        net.update(name,model1.predict(x),weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path='../../datasets/Diabatic_Retinopathy/train/10325_left.jpeg'\n",
    " \n",
    "                    \n",
    "img=Image.open(img_path)\n",
    "x=np.array(img.resize((size1,size2)).convert('RGB')).reshape(1,size2,size1,3).astype('float32')\n",
    "                \n",
    "#img_path='../../datasets/Diabatic_Retinopathy/test.png'\n",
    "#x=resize(imge,True,img_path).astype('float32')\n",
    "#x=np.expand_dims(x,axis=0)\n",
    "#x=preprocess_input(x)\n",
    "print(x.shape)\n",
    "x1=model.predict(x)\n",
    "print(x1)\n",
    "#net=Net()\n",
    "#update(net,model,x1)\n",
    "#points=[[np.argmax(model.predict(x2))]]\n",
    "#print(points)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.getWeights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points=[[np.argmax(model.predict(x2))]]\n",
    "print(points)\n",
    "print(net.blobs[\"add_32\"].data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet(points):\n",
    "    points = layers.fc(points=points, layer='prediction', prevLayer='fc')\n",
    "    points = layers.fc(points=points, layer='fc', prevLayer='avg_pool')\n",
    "    print(points)\n",
    "    points = layers.pool(points=points, prevLayer='res5c_branch2c', K=7, S=7)\n",
    "    blob_list = [ '2a', '2b', '2c', '3a', '3b', '3c','3d', '4a', '4b', '4c', '4d', '4e', '4f','5a', '5b', '5c']\n",
    "    add=['max_pooling2d_2']\n",
    "    \n",
    "    for i in range(17,33):\n",
    "        add.append(str(i))\n",
    "    for i in range(len(blob_list)-1,-1,-1):\n",
    "        print(\"i=\",i)\n",
    "        points = layers.res(points, blob_list[i],add[i])\n",
    "        print(points)\n",
    "    print(points)\n",
    "    points = layers.pool(points=points, prevLayer='conv1',K=3, S=2)\n",
    "   \n",
    "    points = layers.conv(points=points,  layer='conv1', prevLayer=base_model.layers[0].name, K=7, S=2, P=3)\n",
    "    points = layers.data(points=points)\n",
    "    return points\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers=Fixations(net)\n",
    "point=resnet(points)\n",
    "point=np.array(point)\n",
    "print(point.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "def outlier_removal(points, diag):\n",
    "    neighbors = np.zeros((points.shape[0]))\n",
    "    selPoints = np.empty((1, 2))\n",
    "    for i in range(points.shape[0]):\n",
    "        diff = np.sqrt(np.sum(np.square(points-points[i]), axis=1))\n",
    "        neighbors[i] = np.sum(diff < diag)\n",
    "    for i in range(points.shape[0]):\n",
    "        if neighbors[i] > 0.05*points.shape[0]:\n",
    "            selPoints = np.append(selPoints, points[i:i+1, :], axis=0)\n",
    "    selPoints = selPoints[1:, :]\n",
    "    selPoints = selPoints.astype(int)\n",
    "    return selPoints\n",
    "\n",
    "\n",
    "def heatmap(img, points, sigma=20):\n",
    "    k = (np.min(img.shape[:2])) if (\n",
    "        np.min(img.shape[:2]) % 2 == 1) else (np.min(img.shape[:2])-1)\n",
    "    mask = np.zeros(img.shape[:2])\n",
    "    shape = mask.shape\n",
    "    for i in range(points.shape[0]):\n",
    "        # Check if inside the image\n",
    "        if points[i, 0] < shape[0] and points[i, 1] < shape[1]:\n",
    "            mask[points[i, 0], points[i, 1]] += 1\n",
    "    # Gaussian blur the points to get a nice heatmap\n",
    "    blur = cv2.GaussianBlur(mask, (k, k), sigma)\n",
    "    blur = blur*255/np.max(blur)\n",
    "    return blur\n",
    "def visualize(img_path, points, diag_percent, image_label):\n",
    "    img = cv2.imread(img_path)\n",
    "    img= cv2.resize(img, (224, 224)) \n",
    "    b, g, r = cv2.split(img)\n",
    "    img = cv2.merge((r, g, b))\n",
    "    diag = math.sqrt(img.shape[0]**2 + img.shape[1]**2)*diag_percent\n",
    "    values = np.asarray(points)\n",
    "    selPoints = outlier_removal(values, diag)\n",
    "    # Make heatmap and show images\n",
    "    hm = heatmap(np.copy(img), selPoints)\n",
    "    _, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    ax[0].imshow(img), ax[0].axis('off'), ax[0].set_title(image_label)\n",
    "    ax[1].imshow(img), ax[1].axis('off'),\n",
    "    ax[1].scatter(selPoints[:, 1], selPoints[:, 0]),\n",
    "    ax[1].set_title('CNN Fixations')\n",
    "    ax[2].imshow(img), ax[2].imshow(hm, 'jet', alpha=0.4),\n",
    "    ax[2].axis('off'), ax[2].set_title('Heatmap')\n",
    "    plt.show()\n",
    "img_path='../../datasets/Diabatic_Retinopathy/test.png'\n",
    "visualize(img_path, point, diag_percent=0.1, image_label='0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"../../datasets/Diabatic_Retinopathy/rss50features.npy\",features)\n",
    "np.save(\"../../datasets/Diabatic_Retinopathy/rss50label.npy\",labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape(\"../../datasets/Diabatic_Retinopathy/vgg.npy\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=np.load(\"../../datasets/Diabatic_Retinopathy/rss50features.npy\")\n",
    "labels=np.load(\"../../datasets/Diabatic_Retinopathy/rss50label.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=np.random.rand(100)\n",
    "p=p.reshape(1,1,1,1)\n",
    "print(p[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basemodel=ResNet50(weights=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "\n",
    "model.add(basemodel)\n",
    "\n",
    "model.add(Dense(100,name='fc',activation='sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(5,activation='softmax',name='prediction'))\n",
    "model.compile(optimizer='adadelta',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basemodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(image,label,epochs=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q=np.empty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit()\n",
    "model.add(Dense(10,input_shape=(100,)))\n",
    "model.compile(optimizer='adadelta',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "x=np.random.rand(10,100)\n",
    "y=np.random.rand(10,10)\n",
    "model.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=np.load(\"../../datasets/Diabatic_Retinopathy/RSS50features.npy\")\n",
    "label=np.load(\"../../datasets/Diabatic_Retinopathy/label.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "\n",
    "model.add(Dense(5000,name='fc1',input_shape=(2048,)))\n",
    "model.add(Dense(100,name='fc2'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(5,activation='softmax',name='prediction'))\n",
    "model.compile(optimizer='rmsprop',loss='categorical_hinge',metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(features,label,batch_size=100,epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=model.predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.where(features>5)[0])/9834"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "2217949/9834"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "\n",
    "#\n",
    "# image dimensions\n",
    "#\n",
    "\n",
    "img_height = 1000\n",
    "img_width = 1000\n",
    "img_channels = 3\n",
    "\n",
    "#\n",
    "# network params\n",
    "#\n",
    "\n",
    "cardinality = 32\n",
    "\n",
    "\n",
    "def residual_network(x):\n",
    "    \"\"\"\n",
    "    ResNeXt by default. For ResNet set `cardinality` = 1 above.\n",
    "    \n",
    "    \"\"\"\n",
    "    def add_common_layers(y):\n",
    "        y = layers.BatchNormalization()(y)\n",
    "        y = layers.LeakyReLU()(y)\n",
    "\n",
    "        return y\n",
    "\n",
    "    def grouped_convolution(y, nb_channels, _strides):\n",
    "        # when `cardinality` == 1 this is just a standard convolution\n",
    "        if cardinality == 1:\n",
    "            return layers.Conv2D(nb_channels, kernel_size=(3, 3), strides=_strides, padding='same')(y)\n",
    "        \n",
    "        assert not nb_channels % cardinality\n",
    "        _d = nb_channels // cardinality\n",
    "\n",
    "        # in a grouped convolution layer, input and output channels are divided into `cardinality` groups,\n",
    "        # and convolutions are separately performed within each group\n",
    "        groups = []\n",
    "        for j in range(cardinality):\n",
    "            group = layers.Lambda(lambda z: z[:, :, :, j * _d:j * _d + _d])(y)\n",
    "            groups.append(layers.Conv2D(_d, kernel_size=(3, 3), strides=_strides, padding='same')(group))\n",
    "            \n",
    "        # the grouped convolutional layer concatenates them as the outputs of the layer\n",
    "        y = layers.concatenate(groups)\n",
    "\n",
    "        return y\n",
    "\n",
    "    def residual_block(y, nb_channels_in, nb_channels_out, _strides=(1, 1), _project_shortcut=False):\n",
    "        \"\"\"\n",
    "        Our network consists of a stack of residual blocks. These blocks have the same topology,\n",
    "        and are subject to two simple rules:\n",
    "        - If producing spatial maps of the same size, the blocks share the same hyper-parameters (width and filter sizes).\n",
    "        - Each time the spatial map is down-sampled by a factor of 2, the width of the blocks is multiplied by a factor of 2.\n",
    "        \"\"\"\n",
    "        shortcut = y\n",
    "\n",
    "        # we modify the residual building block as a bottleneck design to make the network more economical\n",
    "        y = layers.Conv2D(nb_channels_in, kernel_size=(1, 1), strides=(1, 1), padding='same')(y)\n",
    "        y = add_common_layers(y)\n",
    "\n",
    "        # ResNeXt (identical to ResNet when `cardinality` == 1)\n",
    "        y = grouped_convolution(y, nb_channels_in, _strides=_strides)\n",
    "        y = add_common_layers(y)\n",
    "\n",
    "        y = layers.Conv2D(nb_channels_out, kernel_size=(1, 1), strides=(1, 1), padding='same')(y)\n",
    "        # batch normalization is employed after aggregating the transformations and before adding to the shortcut\n",
    "        y = layers.BatchNormalization()(y)\n",
    "\n",
    "        # identity shortcuts used directly when the input and output are of the same dimensions\n",
    "        if _project_shortcut or _strides != (1, 1):\n",
    "            # when the dimensions increase projection shortcut is used to match dimensions (done by 1Ã—1 convolutions)\n",
    "            # when the shortcuts go across feature maps of two sizes, they are performed with a stride of 2\n",
    "            shortcut = layers.Conv2D(nb_channels_out, kernel_size=(1, 1), strides=_strides, padding='same')(shortcut)\n",
    "            shortcut = layers.BatchNormalization()(shortcut)\n",
    "\n",
    "        y = layers.add([shortcut, y])\n",
    "\n",
    "        # relu is performed right after each batch normalization,\n",
    "        # expect for the output of the block where relu is performed after the adding to the shortcut\n",
    "        y = layers.LeakyReLU()(y)\n",
    "\n",
    "        return y\n",
    "\n",
    "    # conv1\n",
    "    x = layers.Conv2D(64, kernel_size=(7, 7), strides=(2, 2), padding='same')(x)\n",
    "    x = add_common_layers(x)\n",
    "\n",
    "    # conv2\n",
    "    x = layers.MaxPool2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x)\n",
    "    for i in range(3):\n",
    "        project_shortcut = True if i == 0 else False\n",
    "        x = residual_block(x, 128, 256, _project_shortcut=project_shortcut)\n",
    "\n",
    "    # conv3\n",
    "    for i in range(4):\n",
    "        # down-sampling is performed by conv3_1, conv4_1, and conv5_1 with a stride of 2\n",
    "        strides = (2, 2) if i == 0 else (1, 1)\n",
    "        x = residual_block(x, 256, 512, _strides=strides)\n",
    "\n",
    "    # conv4\n",
    "    for i in range(6):\n",
    "        strides = (2, 2) if i == 0 else (1, 1)\n",
    "        x = residual_block(x, 512, 1024, _strides=strides)\n",
    "\n",
    "    # conv5\n",
    "    for i in range(3):\n",
    "        strides = (2, 2) if i == 0 else (1, 1)\n",
    "        x = residual_block(x, 1024, 2048, _strides=strides)\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(1)(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "image_tensor = layers.Input(shape=(img_height, img_width, img_channels))\n",
    "network_output = residual_network(image_tensor)\n",
    "  \n",
    "model = models.Model(inputs=[image_tensor], outputs=[network_output])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basemodel=ResNet50(input_shape=(1500,1500,3),include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(basemodel)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1000,name='fc',activation='sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(5,activation='softmax',name='prediction'))\n",
    "model.compile(optimizer='adadelta',loss='categorical_crossentropy',metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basemodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].layers[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createGenerator():\n",
    "    for i in range(1,4):\n",
    "    \n",
    "        try:\n",
    "            train1=list(os.walk('../../datasets/Diabatic_Retinopathy/train.zip.00'+str(i)))[0][2]\n",
    "            print(len(train1))\n",
    "        \n",
    "            train2=list(map(lambda x:x.split('.')[0],train1))\n",
    "            train.extend(train2)\n",
    "       \n",
    "            for j in train2:\n",
    "                try:\n",
    "                    imge=Image.open('../../datasets/Diabatic_Retinopathy/train.zip.00'+str(i)+'/'+j+'.jpeg')\n",
    "                \n",
    "                \n",
    "\n",
    "mygenerator = createGenerator() # create a generator\n",
    "print(mygenerator) # mygenerator is an object!\n",
    "\n",
    "for i in mygenerator:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,4):\n",
    "    \n",
    "    try:\n",
    "        train1=list(os.walk('../../datasets/Diabatic_Retinopathy/train.zip.00'+str(i)))[0][2]\n",
    "        print(len(train1))\n",
    "        from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()\n",
    "        train2=list(map(lambda x:x.split('.')[0],train1))\n",
    "        train.extend(train2)\n",
    "       \n",
    "        for j in train2:\n",
    "            try:\n",
    "                imge=Image.open('../../datasets/Diabatic_Retinopathy/train.zip.00'+str(i)+'/'+j+'.jpeg')\n",
    "                (x,y)=imge.size\n",
    "                if(mn1>x):\n",
    "                    mn1=x\n",
    "                    mn2=y\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
