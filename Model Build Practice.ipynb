{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GAN\n",
    "\n",
    "z=tf.Variable(np.random.rand(1,3),dtype=tf.float32)\n",
    "\n",
    "x=tf.Variable(np.random.rand(1,10),dtype=tf.float32)\n",
    "\n",
    "def D():\n",
    "    inp=tf.keras.Input(shape=(10,))\n",
    "    x=tf.keras.layers.Dense(3)(inp)\n",
    "    y=tf.keras.layers.Dense(1,activation='sigmoid')(x)\n",
    "    model=tf.keras.Model(inputs=inp,outputs=y)\n",
    "    return model\n",
    "\n",
    "\n",
    "def G():\n",
    "    inp=tf.keras.Input(shape=(3,))\n",
    "    x=tf.keras.layers.Dense(3,activation='relu')(inp)\n",
    "    y=tf.keras.layers.Dense(10)(x)\n",
    "    model=tf.keras.Model(inputs=inp,outputs=y)\n",
    "    return model\n",
    "\n",
    "\n",
    "def train():\n",
    "    cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=1)\n",
    "    print(\"z=\",z)\n",
    "    no_of_epochs=100\n",
    "    k_steps=10\n",
    "    for i in range(no_of_epochs):\n",
    "        with tf.GradientTape( persistent=True, watch_accessed_variables=True) as tap:\n",
    "\n",
    "            real_op=model_d(x)\n",
    "            real_loss=cross_entropy(tf.ones_like(real_op),real_op)\n",
    "            fake_op=model_d(model_g(z))\n",
    "            fake_loss=cross_entropy(tf.zeros_like(fake_op),fake_op)\n",
    "            loss_d=real_loss+fake_loss\n",
    "            if i%k_steps==0:\n",
    "                loss_g=cross_entropy(tf.ones_like(fake_op),fake_op)\n",
    "                #loss_d=tf.math.log(model_d(x))+tf.math.log(1-model_d(model_g(z)))\n",
    "\n",
    "        var_list_d=model_d.trainable_weights\n",
    "        #var_list.extend(model.trainable_weights)\n",
    "\n",
    "        gradients=tap.gradient(loss_d,var_list_d)\n",
    "\n",
    "        #print(\"grad=\",gradients)\n",
    "\n",
    "        if i%k_steps==0:\n",
    "            var_list_g=[z]\n",
    "            #var_list_g.extend(model_g.trainable_weights)\n",
    "\n",
    "\n",
    "            gradients=tap.gradient(loss_g,var_list_g)\n",
    "\n",
    "            print(\"grad=\",gradients)\n",
    "\n",
    "            opt.apply_gradients(zip(gradients, var_list_g))\n",
    "\n",
    "\n",
    "            print(\"z=\",z)\n",
    "            \n",
    "            \n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deep Q Learning\n",
    "\n",
    "class Env:\n",
    "    \n",
    "    def __init__(self,state_dim,action_dim):\n",
    "        self.state_dim=state_dim\n",
    "        self.action_dim=action_dim\n",
    "        self.model=self.model(state_dim,action_dim)\n",
    "        \n",
    "    def step(self,state,action):\n",
    "        act=np.zeros((state.shape[0],self.action_dim))\n",
    "        \n",
    "        i=np.arange(0,state.shape[0])\n",
    "       \n",
    "        act[i,action]=1.0\n",
    "       \n",
    "        return self.model.predict([state,act])\n",
    "        \n",
    "    def model(self,state_dim,action_dim):\n",
    "        state=tf.keras.Input(shape=(state_dim,))\n",
    "        action=tf.keras.Input(shape=(action_dim,))\n",
    "        conc=tf.keras.layers.Concatenate(axis=1)([state, action])\n",
    "        \n",
    "        x=tf.keras.layers.Dense(3,activation='relu')(conc)\n",
    "        next_state=tf.keras.layers.Dense(state_dim,activation='sigmoid')(x)\n",
    "        reward=tf.keras.layers.Dense(1)(x)\n",
    "        model=tf.keras.Model(inputs=[state,action],outputs=[next_state,reward])\n",
    "        return model\n",
    "\n",
    "    \n",
    "\n",
    "def Q(state_dim,action_dim):\n",
    "    inp=tf.keras.Input(shape=(state_dim,))\n",
    "    x=tf.keras.layers.Dense(3,activation='relu')(inp)\n",
    "    y=tf.keras.layers.Dense(action_dim,activation='sigmoid')(x)\n",
    "    model=tf.keras.Model(inputs=inp,outputs=y)\n",
    "    return model\n",
    "\n",
    "\n",
    "state_dim=2\n",
    "action_dim=3\n",
    "q1=Q(state_dim,action_dim)\n",
    "q=Q(state_dim,action_dim)\n",
    "\n",
    "env=Env(2,3)\n",
    "state=np.random.rand(1,2)\n",
    "action=[1]\n",
    "\n",
    "episodes=100\n",
    "k_steps=10\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=1)\n",
    "    \n",
    "\n",
    "for eps in range(episodes):\n",
    "    (next_state,reward)=env.step(state,action)\n",
    "    \n",
    "    with tf.GradientTape(persistent=True, watch_accessed_variables=True) as tap:\n",
    "        \n",
    "        #print(next_state,reward)\n",
    "        predicted=q(state)\n",
    "\n",
    "        target=reward+tf.math.reduce_max(q1(next_state),axis=1) \n",
    "\n",
    "\n",
    "        loss=tf.keras.losses.MSE(target,predicted)\n",
    "    \n",
    "        #print(\"episodes =\",eps)\n",
    "        if eps%k_steps==0:\n",
    "            #print(\"weights updated\")\n",
    "            q1.set_weights(q.get_weights()) \n",
    "\n",
    "        #print(\"target=\",target,\" pred=\",predicted)\n",
    "        \n",
    "        state=next_state\n",
    "    gradients=tap.gradient(loss,q.trainable_weights)\n",
    "\n",
    "    \n",
    "\n",
    "    opt.apply_gradients(zip(gradients, q.trainable_weights))\n",
    "    \n",
    "    #print(\"grad applied\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[0.17339589 0.6973886  0.8148903 ]]\n",
      "\n",
      " [[0.82217306 0.10814424 0.82531536]]\n",
      "\n",
      " [[0.81147045 0.04377539 0.4597268 ]]\n",
      "\n",
      " [[0.6850138  0.07223559 0.8211778 ]]\n",
      "\n",
      " [[0.7916062  0.7285566  0.5754579 ]]\n",
      "\n",
      " [[0.70390385 0.6316571  0.8129708 ]]\n",
      "\n",
      " [[0.8057768  0.36423722 0.8024985 ]]\n",
      "\n",
      " [[0.79037565 0.5037587  0.7234709 ]]\n",
      "\n",
      " [[0.46806952 0.73017585 0.6679684 ]]\n",
      "\n",
      " [[0.80407786 0.7896915  0.5878836 ]]], shape=(10, 1, 3), dtype=float32) tf.Tensor(\n",
      "[[ 0.16754758]\n",
      " [ 0.23071122]\n",
      " [ 0.23336972]\n",
      " [ 0.22892481]\n",
      " [-0.02624834]\n",
      " [ 0.11573507]\n",
      " [ 0.1117183 ]\n",
      " [ 0.10573964]\n",
      " [ 0.13343468]\n",
      " [ 0.07844465]], shape=(10, 1), dtype=float32) tf.Tensor(\n",
      "[[0.6013521]\n",
      " [8.575658 ]\n",
      " [3.2821965]\n",
      " [1.4999222]\n",
      " [8.263745 ]\n",
      " [8.050822 ]\n",
      " [1.5405911]\n",
      " [1.6062477]\n",
      " [0.9716675]\n",
      " [3.1337337]], shape=(10, 1), dtype=float32)\n",
      "tf.Tensor(-75.30272, shape=(), dtype=float32) tf.Tensor(102.49807, shape=(), dtype=float32)\n",
      "p grads= [<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
      "array([[-8.7669525,  0.       ,  0.       ],\n",
      "       [20.699623 ,  0.       ,  0.       ]], dtype=float32)>, <tf.Tensor: shape=(3,), dtype=float32, numpy=array([14.8951235,  0.       ,  0.       ], dtype=float32)>, <tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
      "array([[0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.]], dtype=float32)>, <tf.Tensor: shape=(3,), dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>, <tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
      "array([[-0.8350624 , -0.838825  , -0.84925705],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ]], dtype=float32)>, <tf.Tensor: shape=(3,), dtype=float32, numpy=array([-7.1157465, -7.1547155, -7.26275  ], dtype=float32)>]\n",
      "v grads= [<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
      "array([[-0.58732903,  6.126568  ,  0.        ],\n",
      "       [-0.906053  , 11.635945  ,  0.        ]], dtype=float32)>, <tf.Tensor: shape=(3,), dtype=float32, numpy=array([-1.3846941, 18.147644 ,  0.       ], dtype=float32)>, <tf.Tensor: shape=(3, 1), dtype=float32, numpy=\n",
      "array([[3.3315144],\n",
      "       [8.0883255],\n",
      "       [0.       ]], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([27.587532], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "# actor critic models\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Env:\n",
    "    \n",
    "    def __init__(self,state_dim,action_dim):\n",
    "        self.state_dim=state_dim\n",
    "        self.action_dim=action_dim\n",
    "        #self.model=self.model(state_dim,action_dim)\n",
    "        \n",
    "    def step(self,state,action):\n",
    "        return (np.random.rand(1,self.state_dim),np.random.rand(1)*10,True)\n",
    "        \n",
    "        \n",
    "#     def model(self,state_dim,action_dim):\n",
    "#         state=tf.keras.Input(shape=(state_dim,))\n",
    "#         action=tf.keras.Input(shape=(action_dim,))\n",
    "#         conc=tf.keras.layers.Concatenate(axis=1)([state, action])\n",
    "        \n",
    "#         x=tf.keras.layers.Dense(3,activation='relu',trainable=False)(conc)\n",
    "#         next_state=tf.keras.layers.Dense(state_dim,activation='sigmoid',trainable=False)(x)\n",
    "#         reward=tf.keras.layers.Dense(1,trainable=False)(x)\n",
    "#         done=tf.keras.layers.Dense(1,activation='sigmoid',trainable=False)(x)\n",
    "#         model=tf.keras.Model(inputs=[state,action],outputs=[next_state,reward,done])\n",
    "#         return model\n",
    "\n",
    "    def reset(self):\n",
    "        return np.random.rand(1,self.state_dim)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def policy(state_dim,action_dim):\n",
    "    inp=tf.keras.Input(shape=(state_dim,))\n",
    "    x=tf.keras.layers.Dense(3,activation='relu')(inp)\n",
    "    mu=tf.keras.layers.Dense(action_dim,activation='sigmoid')(x)\n",
    "    sigma=tf.keras.layers.Dense(action_dim,activation='sigmoid')(x)\n",
    "    \n",
    "    dist=tfp.layers.DistributionLambda(lambda y: tfp.distributions.Normal(loc=y[0], scale=y[1]))([mu,sigma])\n",
    "    \n",
    "    model=tf.keras.Model(inputs=inp,outputs=dist)\n",
    "    return model\n",
    "\n",
    "\n",
    "def critic(state_dim):\n",
    "    inp=tf.keras.Input(shape=(state_dim,))\n",
    "    x=tf.keras.layers.Dense(3,activation='relu')(inp)\n",
    "    y=tf.keras.layers.Dense(1)(x)\n",
    "    model=tf.keras.Model(inputs=inp,outputs=y)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def run_episode(max_step):\n",
    "    action_probs = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "    values=tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "    rewards=tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "    state=env.reset()\n",
    "    \n",
    "    for t in range(max_step):\n",
    "\n",
    "        dist=p(state)\n",
    "        \n",
    "        action_sample=tf.squeeze(dist.sample([1]),axis=0)\n",
    "    \n",
    "        (next_state,reward,done)=env.step(state,action_sample)\n",
    "        \n",
    "        action_probs=action_probs.write(t,tf.squeeze(dist.prob(action_sample)))\n",
    "        values=values.write(t,tf.squeeze(v(state)))\n",
    "        \n",
    "        rewards=rewards.write(t,reward)\n",
    "       \n",
    "        state=next_state\n",
    "   \n",
    "    return (action_probs.stack(),values.stack(),rewards.stack())\n",
    "        \n",
    "def compute_loss(action_probs,values,expected_returns):\n",
    "    \n",
    "    action_logs=tf.math.log(action_probs)\n",
    "    advantage=expected_returns-values\n",
    "    actor_loss=-tf.reduce_sum(action_logs*advantage)\n",
    "    critic_loss=tf.reduce_sum(tf.keras.losses.MSE(expected_returns,values))\n",
    "    \n",
    "    return (actor_loss,critic_loss)\n",
    "\n",
    "\n",
    "def expected_return(rewards,gamma,standarize):\n",
    "    n = tf.shape(rewards)[0]\n",
    "    returns = tf.TensorArray(dtype=tf.float32, size=n)\n",
    "\n",
    "    discounted_sum=tf.constant(0.0)\n",
    "    for i in range(n):\n",
    "        discounted_sum=rewards[i]+gamma*discounted_sum\n",
    "        \n",
    "        returns=returns.write(i,discounted_sum)\n",
    "    returns=returns.stack()[::-1]      \n",
    "    if standarize:\n",
    "        returns = ((returns - tf.math.reduce_mean(returns)) /(tf.math.reduce_std(returns) + 0.00001))\n",
    "\n",
    "    return returns    \n",
    "\n",
    "\n",
    "def train_step(opt,max_step,gamma):\n",
    "   \n",
    "       \n",
    "    with tf.GradientTape(persistent=True) as tap :\n",
    "       \n",
    "        (action_probs,values,rewards)=run_episode(max_step)\n",
    "        returns=expected_return(rewards,gamma,True)\n",
    "    \n",
    "        action_probs, values, returns = [\n",
    "            tf.expand_dims(x, 1) for x in [action_probs, values, returns]] \n",
    " \n",
    "        print(action_probs,values,rewards)\n",
    "        (actor_loss,critic_loss)=compute_loss(action_probs,values,returns)\n",
    "    print(actor_loss,critic_loss)\n",
    "    \n",
    "    p_grads=tap.gradient(actor_loss,p.trainable_weights)\n",
    "    v_grads=tap.gradient(critic_loss,v.trainable_weights)\n",
    "    \n",
    "    print(\"p grads=\",p_grads)\n",
    "    print(\"v grads=\",v_grads)\n",
    "    opt.apply_gradients(zip(p_grads,p.trainable_weights))\n",
    "    \n",
    "    opt.apply_gradients(zip(v_grads,v.trainable_weights))\n",
    "\n",
    "\n",
    "def train():\n",
    "\n",
    "    episodes=1\n",
    "    #k_steps=10\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=1)\n",
    "\n",
    "    gamma=0.1\n",
    "    max_step=10\n",
    "     \n",
    "    for i in range(episodes):\n",
    "        train_step(opt,max_step,gamma)\n",
    "\n",
    "\n",
    "        \n",
    "state_dim=2\n",
    "action_dim=3\n",
    "\n",
    "env=Env(state_dim,action_dim)\n",
    "\n",
    "p=policy(state_dim,action_dim)\n",
    "\n",
    "v=critic(state_dim)\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "step() missing 1 required positional argument: 'action'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-bff1244b338c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: step() missing 1 required positional argument: 'action'"
     ]
    }
   ],
   "source": [
    "\n",
    "env=Env(2,3)\n",
    "(next_state,reward)=env.step(np.random.rand(1,2),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 1])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_advantage.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(2,), dtype=float32, numpy=array([4., 6.], dtype=float32)>]\n",
      "<tf.Variable 'Variable:0' shape=(2,) dtype=float32, numpy=array([6., 9.], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "with tf.GradientTape(persistent=True, watch_accessed_variables=True) as tap:\n",
    "    x=tf.Variable(np.array([2,3]),dtype=tf.float32)\n",
    "    y=x*x\n",
    "\n",
    "gradients=tap.gradient(y,[x])\n",
    "print(gradients)\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=1)\n",
    "\n",
    "opt.apply_gradients(zip(gradients,[x]))\n",
    "    \n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method _EagerTensorBase.numpy of <tf.Tensor: shape=(), dtype=int64, numpy=1>>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.losses.MSE(np.array([4]),np.array([3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=int64, numpy=\n",
       "array([[3, 1],\n",
       "       [3, 2]])>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.Variable(np.array([[2,3],[1,3]]))[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.22900645, 0.12034588]], dtype=float32)>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist=p(np.random.rand(1,3))\n",
    "\n",
    "        \n",
    "action_sample=tf.squeeze(dist.sample([1]),axis=0)\n",
    "\n",
    "dist.prob(action_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y= tf.Tensor([1. 1.], shape=(2,), dtype=float32)\n",
      "[<tf.Tensor: shape=(2,), dtype=float32, numpy=array([3., 6.], dtype=float32)>, <tf.Tensor: shape=(2,), dtype=float32, numpy=array([2., 2.], dtype=float32)>]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=() dtype=int64, numpy=1>"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class CustomLayer (tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self,num):\n",
    "        super(CustomLayer,self).__init__()\n",
    "        self.num=num\n",
    "        \n",
    "    def build(self,input_shape):\n",
    "        self.wg=tf.Variable(np.ones((self.num,)),dtype=tf.float32)\n",
    "        \n",
    "    @tf.custom_gradient\n",
    "    def custom_op1(self,x,w):\n",
    "        \n",
    "        result=w\n",
    "        \n",
    "        def custom_grad(dy):\n",
    "            grad1 = dy*x*3.0\n",
    "            grad2=dy*w*2.0\n",
    "            return grad1,grad2\n",
    "        \n",
    "        return result, custom_grad\n",
    "\n",
    "    def call(self,inp):\n",
    "        #print(\"wg===\",self.wg)\n",
    "        return self.custom_op1(inp,self.wg)\n",
    "\n",
    "\n",
    "layer=CustomLayer(2)     \n",
    "x=tf.Variable(np.array([1.0,2.0]),dtype=tf.float32)\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=1)\n",
    "with tf.GradientTape(persistent=True) as tap:\n",
    "    \n",
    "    tap.watch(x)\n",
    "    tap.watch(layer.trainable_weights)\n",
    "    \n",
    "    y=layer(x)\n",
    "    \n",
    "    print(\"y=\",y)\n",
    "    \n",
    "    \n",
    "var_list=[x]\n",
    "var_list.extend(layer.trainable_weights)\n",
    "\n",
    "grads=tap.gradient(y,var_list)\n",
    "\n",
    "\n",
    "print(grads)\n",
    "opt.apply_gradients(zip(grads,var_list))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([4.], shape=(1,), dtype=float64) tf.Tensor([9.], shape=(1,), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "@tf.custom_gradient\n",
    "def f(x,y):\n",
    "    fx = x+y\n",
    "    \n",
    "    def grad(dy):\n",
    "        return dy * 2,dy*3\n",
    "    return fx, grad\n",
    "\n",
    "\n",
    "\n",
    "x=tf.Variable(np.array([2.0]))\n",
    "y=tf.Variable(np.array([2.0]))\n",
    "with tf.GradientTape(persistent=True, watch_accessed_variables=True) as tap:\n",
    "    \n",
    "    p=f(x,y)\n",
    "    q=f(p,y)\n",
    "\n",
    "print(tap.gradient(q,x),tap.gradient(q,y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'custom_layer_43/Variable:0' shape=(2,) dtype=float32, numpy=array([1., 1.], dtype=float32)>]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.trainable_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1=np.random.rand(10,3)\n",
    "x=np.random.rand(1,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(x,w1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
