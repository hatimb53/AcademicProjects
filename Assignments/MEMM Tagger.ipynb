{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'open22'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-a618ad562868>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mio\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mopen22\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'open22'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.classify import MaxentClassifier\n",
    "import pickle\n",
    "import os,sys\n",
    "from io import open22\n",
    "import string\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words=set(stopwords.words('english'))\n",
    "stopwordslist=[]\n",
    "for stop_word in stop_words:\n",
    "    stopwordslist.append(stop_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadDataset(train_filename,testFilename):\n",
    "    training_file = open(train_filename, \"rt\")\n",
    "    testing_file = open(testFilename, \"rt\")\n",
    "    return training_file, testing_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generateTagDict(train):\n",
    "    i=1\n",
    "    tagDict={}\n",
    "    change_of_sentence_flag=0\n",
    "    for line in train:\n",
    "        s = re.match(r'^\\s*$', line)  #find empty line\n",
    "        if(s):\n",
    "            change_of_sentence_flag = 1\n",
    "        else: \n",
    "            sentenceList = line.split()\n",
    "            tag =  sentenceList[1]\n",
    "            #store words that are begining of the sentence\n",
    "            if(change_of_sentence_flag == 1):\n",
    "                change_of_sentence_flag = 0\n",
    "            if(tag not in  tagDict):\n",
    "                tagDict[tag]= i\n",
    "                i=i+1\n",
    "    return tagDict\n",
    "\n",
    "\n",
    "def MEMM_features(word ,previous_tag, tagIntegerMap):\n",
    "    stemmer = PorterStemmer() \n",
    "    features = []\n",
    "    #checking for upper case\n",
    "    if(word[0].isupper()):\n",
    "        features.append(1)\n",
    "    else:\n",
    "        features.append(0)\n",
    "    #Checking whether word is start word \n",
    "    if(word in wordStartList):\n",
    "        features.append(1)\n",
    "    else:\n",
    "        features.append(0)\n",
    "        \n",
    "    #Checking for word stating with capital character and not a start word\n",
    "    if(word not in wordStartList and word[0].isupper()):\n",
    "        features.append(1)\n",
    "    else:\n",
    "        features.append(0)\n",
    "     \n",
    "    try:\n",
    "        if(float(word)):\n",
    "            features.append(1)\n",
    "    except:\n",
    "        features.append(0)\n",
    "    #features['term_frequency'] = my_dict[word]\n",
    "    \n",
    "    #Appending previous_tag in feature set\n",
    "    if(previous_tag in tagIntegerMap):\n",
    "        features.append(tagIntegerMap[previous_tag])\n",
    "    else:\n",
    "         features.append(0)\n",
    "            \n",
    "    #check if word ends with ing\n",
    "    if(word.endswith('ing')):\n",
    "        features.append(1)\n",
    "    else:\n",
    "        features.append(0)\n",
    "    #check if word ends with ed\n",
    "    if(word.endswith('ed')):\n",
    "        features.append(1)\n",
    "    else:\n",
    "        features.append(0)\n",
    "    #check is word ends wih ly\n",
    "    if(word.endswith('ly')):\n",
    "        features.append(1)\n",
    "    else:\n",
    "        features.append(0)\n",
    "    \n",
    "    #word length\n",
    "    features.append(len(word))\n",
    "    #check if word is a stop word:\n",
    "    if(word in stopwordslist):\n",
    "        features.append(1)\n",
    "    else:\n",
    "        features.append(0)\n",
    "        \n",
    "    #check if word is a punctuation:\n",
    "    if(word in string.punctuation):\n",
    "        features.append(1)\n",
    "    else:\n",
    "        features.append(0)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_of_sentence_flag = 0 #a marker for the end of sentence\n",
    "wordStartList = [] #store words that are begining of the sentence\n",
    "training,testing=loadDataset(\"train.txt\",\"test.txt\")\n",
    "tagDict = generateTagDict(training)\n",
    "training = open(\"train.txt\", \"rt\")\n",
    "train_features = generateMatrix(training)\n",
    "len(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_labeled_featuresets = [(MEMM_features(word,previous_tag,tagDict),tagDict[tag]) for (word,previous_tag,tag) in train_features]\n",
    "trainSet=[]\n",
    "y_train = []\n",
    "for i in range(len(train_labeled_featuresets)):\n",
    "    trainSet.append(train_labeled_featuresets[i][0])\n",
    "    y_train.append(train_labeled_featuresets[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_features = generateMatrix(testing)\n",
    "test_labeled_featuresets = [(MEMM_features(word,previous_tag,tagDict),tagDict[tag]) for (word,previous_tag,tag) in test_features]\n",
    "testSet=[]\n",
    "y_test = []\n",
    "for i in range(len(test_labeled_featuresets)):\n",
    "    testSet.append(test_labeled_featuresets[i][0])\n",
    "    y_test.append(test_labeled_featuresets[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import math\n",
    "\n",
    "classifier = LogisticRegression(multi_class='multinomial', solver='saga')\n",
    "classifier.fit(trainSet,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MEMM(wordList):\n",
    "    viterbi = [[0 for x in range(len(wordList))] for x in range(len(tagDict))] \n",
    "    backpointer = [['' for x in range(len(wordList))] for x in range(len(tagDict))] \n",
    "    '''startWord=wordList[0]\n",
    "    featureVector = MEMM_features(startWord,0,tagDict)\n",
    "    #print(type(featureVector))\n",
    "    startProbablity=classifier.predict_proba([featureVector])\n",
    "    for i in range(len(tagDict)):\n",
    "        viterbi[i][0]= startProbablity[0][i]'''\n",
    "        \n",
    "    for wordIndex in range(len(wordList)):\n",
    "        for classIndex in range(len(classifier.classes_)):\n",
    "            if(wordIndex==0):\n",
    "                featureVector = MEMM_features(wordList[wordIndex],0,tagDict)\n",
    "            else:\n",
    "                featureVector = MEMM_features(wordList[wordIndex],classifier.classes_[classIndex],tagDict)\n",
    "            probablity=classifier.predict_proba([featureVector])\n",
    "            if(wordIndex!=0):\n",
    "                for i in range(len(tagDict)):\n",
    "                    if(viterbi[i][wordIndex] < probablity[0][i] * viterbi[classIndex][wordIndex-1]):\n",
    "                        viterbi[i][wordIndex]= probablity[0][i] * viterbi[classIndex][wordIndex-1]\n",
    "                        #index of tag at which maximum value is obtained\n",
    "                        backpointer[i][wordIndex] = classIndex\n",
    "            else:\n",
    "                for i in range(len(tagDict)):\n",
    "                    viterbi[i][0]= probablity[0][i]\n",
    "                    backpointer[i][0]=0\n",
    "    \n",
    "    maxValue = 0\n",
    "    backIndex=0\n",
    "    for index in range(len(classifier.classes_)):\n",
    "        if(maxValue < viterbi[index][len(wordList)-1]):\n",
    "            maxValue = viterbi[index][len(wordList)-1]\n",
    "            backIndex = index\n",
    "   \n",
    "    predictedTagList=[]\n",
    "    predictedTagList.append(backIndex+1)\n",
    "    for wordIndex in range(len(wordList)-1,0,-1):\n",
    "        backIndex = backpointer[backIndex][wordIndex]\n",
    "        predictedTagList.append(backIndex+1)\n",
    "   \n",
    "    # returning the predicted tags for a particular sentence\n",
    "    return predictedTagList[::-1]\n",
    "    \n",
    "    \n",
    "\n",
    "    #for j in range(len(wordList)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordList = [] #store words in a sentence\n",
    "tagList = [] #store part-of-speech tag in a sentence \n",
    "#prob_table = {} #stpre the posterior\n",
    "y_pred=[]\n",
    "input_file = open('test.txt', \"rt\")\n",
    "for line in input_file:\n",
    "    if line.strip() != '': #if not empty do following \n",
    "        sentenceList = line.split()\n",
    "        word = sentenceList[0]\n",
    "        tag = sentenceList[1]\n",
    "        wordList.append(word)\n",
    "        tagList.append(tag)\n",
    "        if change_of_sentence_flag == 1:\n",
    "            wordStartList.append(word)\n",
    "            change_of_sentence_flag = 0\n",
    "    s = re.match(r'^\\s*$', line)  #find empty line\n",
    "    if s:\n",
    "        #print (wordList)\n",
    "        change_of_sentence_flag = 1\n",
    "        predictedTagList = MEMM(wordList) #list of tagsreturned by HMM function call\n",
    "        #for i in range(len(wordList)): #part_of_speech_tag(tagList) and token_list(wordList) has the same length\n",
    "         #   print(wordList[i]+\" \"+ tagList[i]+ \" \" + \" \" + path[i] + \"\\n\")\n",
    "        y_pred= y_pred + predictedTagList\n",
    "        wordList = [] # refresh word list\n",
    "        tagList = []\n",
    "        boiList = []\n",
    "        #prob_table = {}#refresh prob_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print('accuracy is : ',accuracy_score(y_test,y_pred)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "predictedTagList=MEMM(wordList)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
